{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import gc\n"
      ],
      "metadata": {
        "id": "g7aeZZB4YBWt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 50\n",
        "model_name = \"gpt2\"\n",
        "# model = AutoModelCausalLM.from_pretrained('gpt2')\n",
        "encode_model = GPT2Model.from_pretrained('distilgpt2', output_hidden_states=True)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# recon_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# recon_model = GPT2LMHeadModel.from_pretrained(\"./finetuned_gpt2_embeddings\")\n",
        "# recon_tokenizer = GPT2Tokenizer.from_pretrained(\"./finetuned_gpt2_embeddings\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "encode_model.to(device)\n",
        "# recon_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqp2xbTUYERS",
        "outputId": "b3aeca7c-f3f7-44d7-8840-d53df3c53852"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0-5): 6 x GPT2Block(\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): GPT2Attention(\n",
              "        (c_attn): Conv1D(nf=2304, nx=768)\n",
              "        (c_proj): Conv1D(nf=768, nx=768)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): GPT2MLP(\n",
              "        (c_fc): Conv1D(nf=3072, nx=768)\n",
              "        (c_proj): Conv1D(nf=768, nx=3072)\n",
              "        (act): NewGELUActivation()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yJTcEry1X06y"
      },
      "outputs": [],
      "source": [
        "def embed(text):\n",
        "    input_ids = tokenizer(text,\n",
        "                          return_tensors=\"pt\",\n",
        "                          padding=\"max_length\",\n",
        "                          truncation=True,\n",
        "                          max_length=MAX_LENGTH)['input_ids']\n",
        "    input_ids = input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        embeddings = encode_model(input_ids).last_hidden_state\n",
        "\n",
        "    del input_ids\n",
        "    gc.collect()\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure model is in train mode\n",
        "model.train()\n",
        "\n",
        "# Custom dataset class\n",
        "class EmbeddingTextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, embeddings, texts, tokenizer):\n",
        "        self.embeddings = embeddings  # Precomputed input embeddings (tensor shape: [num_samples, seq_len, hidden_dim])\n",
        "        self.texts = texts  # Corresponding text from tweets\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_embedding = self.embeddings[idx]  # Extract precomputed embedding\n",
        "        text = self.texts[idx]\n",
        "        tokenized_output = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "        return {\n",
        "            \"inputs_embeds\": input_embedding.squeeze(0),\n",
        "            \"labels\": tokenized_output[\"input_ids\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "# Load both datasets\n",
        "anti_brexit_df = pd.read_csv(\"./TweetDataset_AntiBrexit_Jan-Mar2022.csv\")\n",
        "pro_brexit_df = pd.read_csv(\"./TweetDataset_ProBrexit_Jan-Mar2022.csv\")\n",
        "\n",
        "# Extract the relevant text column\n",
        "anti_brexit_texts = anti_brexit_df[\"Hit Sentence\"].dropna().tolist()\n",
        "pro_brexit_texts = pro_brexit_df[\"Hit Sentence\"].dropna().tolist()\n",
        "\n",
        "# Combine both datasets\n",
        "texts = anti_brexit_texts + pro_brexit_texts\n",
        "\n",
        "print(f\"Loaded {len(texts)} tweets.\")\n",
        "\n",
        "# Generate embeddings\n",
        "# embeddings = [embed(text) for text in tqdm(texts, desc=\"Generating Embeddings\", leave=False)]\n",
        "embeddings = list(map(embed, tqdm(texts, desc=\"Generating Embeddings\", leave=False)))\n",
        "# embeddings = list(map(embed, texts))\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "dataset = EmbeddingTextDataset(embeddings, texts, tokenizer)\n",
        "# def pickle_data(embeddings, texts):\n",
        "def pickle_data(dataset):\n",
        "    # dataset = {\n",
        "    #     \"embeddings\": embeddings,\n",
        "    #     \"texts\": texts\n",
        "    # }\n",
        "\n",
        "    # Save as a pickle file\n",
        "    with open(\"synthetic_tweet_embeddings.pkl\", \"wb\") as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "    print(\"Saved dataset to synthetic_tweet_embeddings.pkl\")\n",
        "\n",
        "pickle_data(dataset)\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_embedding_finetune\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_gpt2_embeddings\")\n",
        "tokenizer.save_pretrained(\"./finetuned_gpt2_embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl4UdjPtX8re",
        "outputId": "ae6d7cf1-bacb-408f-c12f-40bdbe322f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 358205 tweets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Embeddings:  11%|â–ˆ         | 38609/358205 [3:44:41<36:03:26,  2.46it/s]"
          ]
        }
      ]
    }
  ]
}